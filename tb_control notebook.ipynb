{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb54ce13-7e18-4975-9b7d-42a8e3951672",
     "showTitle": true,
     "title": "Fetching parameters from ADF pipeline"
    }
   },
   "outputs": [],
   "source": [
    "System_Id=dbutils.widgets.get(\"system_id\")\n",
    "Pipeline_name=dbutils.widgets.get(\"Pipeline_name\")\n",
    "Pipeline_id=dbutils.widgets.get(\"Pipeline_id\")\n",
    "Start=dbutils.widgets.get(\"Start\")\n",
    "End=dbutils.widgets.get(\"End\")\n",
    "Status=dbutils.widgets.get(\"Status\")\n",
    "Duration=dbutils.widgets.get(\"Duration\")\n",
    "Trigger_Name=dbutils.widgets.get(\"Trigger_Name\")\n",
    "Trigger_Type=dbutils.widgets.get(\"Trigger_Type\")\n",
    "Trigger_By=dbutils.widgets.get(\"Trigger_By\")\n",
    "Records_Read=dbutils.widgets.get(\"Records_Read\")\n",
    "Records_Inserted=dbutils.widgets.get(\"Records_Inserted\")\n",
    "Frequency=dbutils.widgets.get(\"Frequency\")\n",
    "Log_File=dbutils.widgets.get(\"Log_File\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e76261e1-1ea3-454d-9718-f5e70e2c9b60",
     "showTitle": true,
     "title": "Typecasting variables"
    }
   },
   "outputs": [],
   "source": [
    "System_Id=int(System_Id)\n",
    "Records_Read=int(Records_Read)\n",
    "Records_Inserted=int(Records_Inserted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e5fe371-1168-49c6-9d72-d3f03a07816f",
     "showTitle": true,
     "title": "Tb_control Schema"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, LongType, TimestampType\n",
    "schema = StructType([\n",
    "         StructField('Control_Id', LongType(), True),\n",
    "         StructField('System_Id', LongType(), True),\n",
    "         StructField('Pipeline_Name', StringType(), True),\n",
    "         StructField('Pipeline_Id', StringType(), True),\n",
    "         StructField('Start', StringType(), True),\n",
    "         StructField('End', StringType(), True),\n",
    "         StructField('Status', StringType(), True),\n",
    "         StructField('Duration', StringType(), True),\n",
    "         StructField('Trigger_Name', StringType(), True),\n",
    "         StructField('Trigger_Type', StringType(), True),\n",
    "         StructField('Triggered_By', StringType(), True),\n",
    "         StructField('Records_Read', LongType(), True),\n",
    "         StructField('Records_Inserted', LongType(), True),\n",
    "         StructField('Frequency', StringType(), True),\n",
    "         StructField('Log_File', StringType(), True),\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f34f83d5-1cb4-43f0-b38f-661af99c090b",
     "showTitle": true,
     "title": "Getting Control_Id"
    }
   },
   "outputs": [],
   "source": [
    "control_id=0\n",
    "try:\n",
    "    control_id=spark.sql(\"select max(Control_Id) from bcp_control.tb_control\").collect()[0][0]+1\n",
    "except Exception as e:\n",
    "    print(\"NO data avialable\")\n",
    "    control_id=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78312440-2c05-4d90-a6d9-c1752fe9050d",
     "showTitle": true,
     "title": "Creating dataframe "
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "df=[(control_id,System_Id,Pipeline_name,Pipeline_id,Start,End,Status,Duration,Trigger_Name,Trigger_Type,Trigger_By,Records_Read,Records_Inserted,Frequency,Log_File)]\n",
    "df=spark.createDataFrame(df,schema=schema)\n",
    "df = df.withColumn(\"Start\", F.to_timestamp(F.col(\"Start\").cast(TimestampType())))\n",
    "df = df.withColumn(\"End\", F.to_timestamp(F.col(\"End\").cast(TimestampType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27b7d0b0-36b2-4623-885b-7354dae082f2",
     "showTitle": true,
     "title": "Sorting data and storing in control table"
    }
   },
   "outputs": [],
   "source": [
    "data=spark.sql(\"select * from bcp_control.tb_control order by Control_Id asc\")\n",
    "data=data.unionByName(df)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "400c5b7e-df28-4465-bf6a-2570b4500a32",
     "showTitle": true,
     "title": "Inserting logs into tb_control"
    }
   },
   "outputs": [],
   "source": [
    "def insert_data(data,TableName):\n",
    "    data.write.mode(\"overwrite\").saveAsTable(TableName)\n",
    "insert_data(data,\"spark_catalog.bcp_control.tb_control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "deef16af-00c7-4971-94bb-028609df8899",
     "showTitle": true,
     "title": "Returning notebook o/p to ADF"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(control_id)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "tb_control notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
